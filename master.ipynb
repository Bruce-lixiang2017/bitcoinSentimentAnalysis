{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done scraping bloomberg\n",
      "done scraping coindesk market\n",
      "done scraping coindesk\n",
      "done scraping marketwatch\n",
      "done scraping bitcoin news\n",
      "[(0, u'0.019*\"etf\" + 0.012*\"sec\" + 0.007*\"company\" + 0.007*\"government\" + 0.006*\"mainstream\"'), (1, u'0.022*\"new\" + 0.010*\"china\" + 0.009*\"transaction\" + 0.008*\"network\" + 0.008*\"launch\"'), (2, u'0.027*\"exchange\" + 0.016*\"bank\" + 0.014*\"mining\" + 0.013*\"new\" + 0.010*\"bitcoin\"'), (3, u'0.022*\"price\" + 0.011*\"get\" + 0.008*\"pay\" + 0.007*\"market\" + 0.007*\"set\"'), (4, u'0.019*\"market\" + 0.017*\"high\" + 0.014*\"time\" + 0.012*\"price\" + 0.008*\"say\"')]\n"
     ]
    }
   ],
   "source": [
    "import requests      \n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import re, csv, string \n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "keywords = ['bitcoin','cryptocurrency','cryptocurrencies', 'crypto', 'blockchain']\n",
    "\n",
    "# wordnet - treebank map tagging system function\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    # if pos tag starts with 'J'\n",
    "    if pos_tag.startswith('J'):\n",
    "        # return wordnet tag \"ADJ\"\n",
    "        return wordnet.ADJ\n",
    "    # if pos tag starts with 'V'\n",
    "    elif pos_tag.startswith('V'):\n",
    "        # return wordnet tag \"VERB\"\n",
    "        return wordnet.VERB\n",
    "    # if pos tag starts with 'N'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        # return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # be default, return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    \n",
    "# takes a string (document) as input and strips all tokens down to their root (lemmatized) form\n",
    "def lemmatize(document):\n",
    "    pattern=r'[a-zA-Z]+[a-zA-Z\\-\\.]'      \n",
    "    tokens = nltk.regexp_tokenize(document, pattern)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # get lemmatized tokens\n",
    "    # lemmatize every word in tagged_tokens\n",
    "    le_words=[wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(tag)) \\\n",
    "              # tagged_tokens is a list of tuples (word, tag)\n",
    "              for (word, tag) in tagged_tokens \\\n",
    "              # remove stop words\n",
    "              if word not in stop_words and \\\n",
    "              # remove punctuations\n",
    "              word not in string.punctuation]\n",
    "    le_words = list(set(le_words))\n",
    "    return le_words\n",
    "\n",
    "def get_headline_list(filtered_data):\n",
    "    headlines_list = []\n",
    "    for row in filtered_data:\n",
    "        headlines_list.append(row[0])\n",
    "    return headlines_list\n",
    "\n",
    "def tokenize_date(s):\n",
    "    pattern=r'[a-z{3}]+[\\.\\s]+[\\d{1,2}\\,\\s]+[\\d{4}]+'                        \n",
    "    tokens=nltk.regexp_tokenize(s, pattern)\n",
    "    date = tokens\n",
    "    return date;\n",
    "\n",
    "def remove_keywords(data):\n",
    "    for row in data:\n",
    "        headline_test = row[0]\n",
    "        for word in headline_test:\n",
    "            if word in keywords:\n",
    "                headline_test.remove(word)\n",
    "    return data\n",
    "\n",
    "def get_bloomberg_headlines():\n",
    "    headlines=[]  # list variable to store headlines\n",
    "    dates = []    # list variable to store dates\n",
    "    page_number = 1\n",
    "#     print 'scraping page %s' % page_number\n",
    "    page_url=\"https://www.bloomberg.com/search?query=bitcoin&sort=time:desc&endTime=2017-10-13T04:36:58.003Z&page=\"+str(page_number)\n",
    "    # loop until page 76\n",
    "    while page_url!=\"https://www.bloomberg.com/search?query=bitcoin&sort=time:desc&endTime=2017-10-13T04:36:58.003Z&page=76\":\n",
    "        page_url=\"https://www.bloomberg.com/search?query=bitcoin&sort=time:desc&endTime=2017-10-13T04:36:58.003Z&page=\"+str(page_number)        \n",
    "        page = requests.get(page_url) \n",
    "        page_number += 1\n",
    "#         print 'scraping page %s' % page_number\n",
    "        if page.status_code!=200:  \n",
    "            page_number = 76 #if page status code fails to equal 200, connection failed; set page_num to while loop condition\n",
    "        else:                   \n",
    "            soup = BeautifulSoup(page.content, 'html.parser')                        \n",
    "            \n",
    "#-----------scrape and clean all headlines and append to a list called headlines----------------------------------------------\n",
    "            for header in soup.find_all('h1', class_ ='search-result-story__headline'):\n",
    "                headline = header.get_text().lower()\n",
    "                headlines.append(lemmatize(headline))\n",
    "                \n",
    "#-----------scrape all dates and append to a list called dates---------------------------------------------------------------\n",
    "            for date in soup.find_all('time', class_ = 'published-at'):\n",
    "                date_published = date.get_text()\n",
    "                dates.append(date_published)\n",
    "                 \n",
    "#---join headlines list and dates list into a list of tuples called data-----------------------------------------------------\n",
    "    data = zip(headlines, dates)\n",
    "\n",
    "#---remove headlines without these keywords----------------------------------------------------------------------------------\n",
    "    filtered_data = filter(lambda x: any(word in x[0] for word in keywords), data)\n",
    "    \n",
    "# this removes the above keywords from each headline to improve performance of the clustering algorithm\n",
    "    for row in filtered_data:\n",
    "        headline_test = row[0]\n",
    "        for word in headline_test:\n",
    "            if word in keywords:\n",
    "                headline_test.remove(word)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "def coindesk_market_parser():\n",
    "    headlines=[]\n",
    "    page_url= \"https://www.coindesk.com/category/markets-news/markets-markets-news/markets-bitcoin/\"\n",
    "    while page_url!=None:\n",
    "        page = requests.get(page_url) \n",
    "        if page.status_code!=200:\n",
    "             page_url=None\n",
    "        else:\n",
    "            all_data = []\n",
    "            for num in range(1,20):\n",
    "                page_url = \"https://www.coindesk.com/category/markets-news/markets-markets-news/markets-bitcoin/page/\"+str(num)+\"/\"\n",
    "                page = requests.get(page_url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                titles = soup.find_all(\"a\", class_ = \"fade\")\n",
    "                titles_dictionary = []\n",
    "                for i in titles:\n",
    "                    title = i.get_text().lower()\n",
    "                    titles_dictionary.append(lemmatize(title))\n",
    "                dates = soup.find_all(\"time\")\n",
    "                dates_list = []\n",
    "                for i in dates:\n",
    "                    date = i.get_text().lower()\n",
    "                    date = date.replace(' at', '')\n",
    "                    date = datetime.strptime(date, '%b %d, %Y %H:%M')\n",
    "                    date = datetime.strftime(date,'%b-%d-%Y')\n",
    "                    dates_list.append(date)\n",
    "                data = zip(titles_dictionary,dates_list)  \n",
    "                all_data.extend(data)\n",
    "\n",
    "        page_url = None\n",
    "    return all_data\n",
    "\n",
    "def coindesk_parser():    \n",
    "    headlines=[]\n",
    "    page_url= \"https://www.coindesk.com/category/technology-news/bitcoin/\"\n",
    "    while page_url!=None:\n",
    "        page = requests.get(page_url) \n",
    "        if page.status_code!=200:\n",
    "             page_url=None\n",
    "        else:\n",
    "            all_data = []\n",
    "            for num in range(1,50):\n",
    "                page_url = \"https://www.coindesk.com/category/technology-news/bitcoin/page/\"+str(num)+\"/\"\n",
    "                page = requests.get(page_url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                titles = soup.find_all(\"a\", class_ = \"fade\")\n",
    "                titles_dictionary = []\n",
    "                for i in titles:\n",
    "                    title = i.get_text().lower()\n",
    "                    titles_dictionary.append(lemmatize(title))\n",
    "                dates = soup.find_all(\"time\")\n",
    "                dates_list = []\n",
    "                for i in dates:\n",
    "                    date = i.get_text().lower()\n",
    "                    date = date.replace(' at', '')\n",
    "                    date = datetime.strptime(date, '%b %d, %Y %H:%M')\n",
    "                    date = datetime.strftime(date,'%b-%d-%Y')\n",
    "                    dates_list.append(date)\n",
    "                data = zip(titles_dictionary,dates_list)  \n",
    "                all_data.extend(data)\n",
    "        page_url = None\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def marketwatch_parser():\n",
    "    headlines=[]\n",
    "    page_url= \"https://www.marketwatch.com/search?q=bitcoin&m=Keyword&rpp=500&mp=806&bd=false&bdv=&rs=false\"\n",
    "    while page_url!=None:\n",
    "        page = requests.get(page_url) \n",
    "        if page.status_code!=200:\n",
    "             page_url=None\n",
    "        else:\n",
    "            all_data = []\n",
    "            page_url = \"https://www.marketwatch.com/search?q=bitcoin&m=Keyword&rpp=500&mp=806&bd=false&bdv=&rs=false\"\n",
    "            page = requests.get(page_url)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            divs = soup.find_all(\"div\", class_ = \"searchresult\")\n",
    "            titles_dictionary = []\n",
    "            for idx, div in enumerate(divs):\n",
    "                titles = div.select(\"a\")\n",
    "                if titles != []:\n",
    "                    title = titles[0].get_text().lower()\n",
    "                    titles_dictionary.append(lemmatize(title))\n",
    "            divs_dates = soup.find_all(\"div\", class_ = \"deemphasized\")\n",
    "            dates_list = []\n",
    "            for idx, div in enumerate(divs_dates):\n",
    "                date = div.get_text().lower()\n",
    "                date = str(tokenize_date(date)[0])\n",
    "                dates_list.append(date)\n",
    "            data = zip(titles_dictionary,dates_list)  \n",
    "            all_data.extend(data)\n",
    "        page_url = None\n",
    "\n",
    "        filtered_data = filter(lambda x: any(word in x[0] for word in keywords), all_data)\n",
    "    return filtered_data\n",
    "\n",
    "def get_bitcoinNews_headlines():\n",
    "    headlines=[]  # list variable to store headlines\n",
    "    dates = []    # list variable to store dates\n",
    "    page_number = 1\n",
    "#     print 'scraping page %s' % page_number\n",
    "    page_url=\"https://news.bitcoin.com/page/\"+str(page_number)+\"/?s=bitcoin\"\n",
    "    # page 115 as parameter\n",
    "    while page_url!=\"https://news.bitcoin.com/page/115/?s=bitcoin\":\n",
    "        page_url=\"https://news.bitcoin.com/page/\"+str(page_number)+\"/?s=bitcoin\"        \n",
    "        page = requests.get(page_url) \n",
    "        page_number += 1\n",
    "#         print 'scraping page %s' % page_number\n",
    "        if page.status_code!=200:  \n",
    "            page_number = None #if page status code fails to equal 200, connection failed; set page_num to while loop condition\n",
    "        else:                   \n",
    "            soup = BeautifulSoup(page.content, 'html.parser')                        \n",
    "            \n",
    "#-----------scrape and clean all headlines and append to a list called headlines----------------------------------------------\n",
    "            main_content = soup.find('div', class_ = 'td-ss-main-content')            \n",
    "            h3s = main_content.find_all('h3', class_ = \"entry-title td-module-title\")\n",
    "            for idx, h3 in enumerate(h3s):\n",
    "                header = h3.select('a')\n",
    "                if header != []:\n",
    "                    headline = header[0].get_text().lower()\n",
    "                    headlines.append(lemmatize(headline))\n",
    "\n",
    "#-----------scrape all dates and append to a list called dates---------------------------------------------------------------\n",
    "            span_dates = main_content.find_all(\"span\", class_ = \"td-post-date\")\n",
    "            for idx, span in enumerate(span_dates):\n",
    "                dates_list = span.select('time')\n",
    "                if dates_list != []:\n",
    "                    date = dates_list[0].get_text()\n",
    "                    dates.append(date)\n",
    "    \n",
    "#---join headlines list and dates list into a list of tuples called data-----------------------------------------------------\n",
    "    data = zip(headlines, dates)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bloomberg = get_bloomberg_headlines()\n",
    "    print(\"done scraping bloomberg\")\n",
    "    coindesk_market = coindesk_market_parser()\n",
    "    print(\"done scraping coindesk market\")\n",
    "    coindesk = coindesk_parser()\n",
    "    print(\"done scraping coindesk\")\n",
    "    marketwatch = marketwatch_parser()\n",
    "    print(\"done scraping marketwatch\")\n",
    "    bitcoinNews = get_bitcoinNews_headlines()\n",
    "    print(\"done scraping bitcoin news\")\n",
    "\n",
    "    all_data = []\n",
    "    all_data.extend(bloomberg + coindesk_market + coindesk + marketwatch + bitcoinNews)\n",
    "    all_data = remove_keywords(all_data)\n",
    "#     print(all_data)\n",
    "    \n",
    "    headline_list = get_headline_list(all_data)\n",
    "    dictionary = corpora.Dictionary(headline_list)\n",
    "    corpus = [dictionary.doc2bow(row) for row in headline_list]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=20)\n",
    "    print(ldamodel.print_topics(num_topics=5, num_words=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
