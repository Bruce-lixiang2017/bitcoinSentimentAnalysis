{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page 1\n",
      "scraping page 2\n",
      "scraping page 3\n",
      "scraping page 4\n",
      "scraping page 5\n",
      "scraping page 6\n",
      "scraping page 7\n",
      "scraping page 8\n",
      "scraping page 9\n",
      "scraping page 10\n",
      "scraping page 11\n",
      "scraping page 12\n",
      "scraping page 13\n",
      "scraping page 14\n",
      "scraping page 15\n",
      "scraping page 16\n",
      "scraping page 17\n",
      "scraping page 18\n",
      "scraping page 19\n",
      "scraping page 20\n",
      "scraping page 21\n",
      "scraping page 22\n",
      "scraping page 23\n",
      "scraping page 24\n",
      "scraping page 25\n",
      "scraping page 26\n",
      "scraping page 27\n",
      "scraping page 28\n",
      "scraping page 29\n",
      "scraping page 30\n",
      "scraping page 31\n",
      "scraping page 32\n",
      "scraping page 33\n",
      "scraping page 34\n",
      "scraping page 35\n",
      "scraping page 36\n",
      "scraping page 37\n",
      "scraping page 38\n",
      "scraping page 39\n",
      "scraping page 40\n",
      "scraping page 41\n",
      "scraping page 42\n",
      "scraping page 43\n",
      "scraping page 44\n",
      "scraping page 45\n",
      "scraping page 46\n",
      "scraping page 47\n",
      "scraping page 48\n",
      "scraping page 49\n",
      "scraping page 50\n",
      "scraping page 51\n",
      "scraping page 52\n",
      "scraping page 53\n",
      "scraping page 54\n",
      "scraping page 55\n",
      "scraping page 56\n",
      "scraping page 57\n",
      "scraping page 58\n",
      "scraping page 59\n",
      "scraping page 60\n",
      "scraping page 61\n",
      "scraping page 62\n",
      "scraping page 63\n",
      "scraping page 64\n",
      "scraping page 65\n",
      "scraping page 66\n",
      "scraping page 67\n",
      "scraping page 68\n",
      "scraping page 69\n",
      "scraping page 70\n",
      "scraping page 71\n",
      "scraping page 72\n",
      "scraping page 73\n",
      "scraping page 74\n",
      "scraping page 75\n",
      "scraping page 76\n",
      "scraping page 77\n",
      "356 headlines were scraped; stopwords were removed; data written to csv file\n"
     ]
    }
   ],
   "source": [
    "import requests                   \n",
    "from bs4 import BeautifulSoup  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import csv   \n",
    "\n",
    "def tokenize(string):\n",
    "    pattern=r'[a-zA-Z]+[a-zA-Z\\-\\.]'                        \n",
    "    tokens=nltk.regexp_tokenize(string, pattern)\n",
    "    return tokens\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def get_headlines():\n",
    "    headlines=[]  # list variable to store headlines\n",
    "#     headlines_dictionary = []\n",
    "    dates = []    # list variable to store dates\n",
    "    page_number = 1\n",
    "    print 'scraping page %s' % page_number\n",
    "    page_url=\"https://www.bloomberg.com/search?query=bitcoin&sort=time:desc&endTime=2017-10-13T04:36:58.003Z&page=\"+str(page_number)\n",
    "    while page_url!=\"https://www.bloomberg.com/search?query=bitcoin&sort=time:desc&endTime=2017-10-13T04:36:58.003Z&page=76\":\n",
    "        page_url=\"https://www.bloomberg.com/search?query=bitcoin&sort=time:desc&endTime=2017-10-13T04:36:58.003Z&page=\"+str(page_number)        \n",
    "        page = requests.get(page_url) \n",
    "        page_number += 1\n",
    "        print 'scraping page %s' % page_number\n",
    "        if page.status_code!=200:  \n",
    "            page_number = 76 #if page status code fails to equal 200, connection failed; set page_num to while loop condition\n",
    "        else:                   \n",
    "            soup = BeautifulSoup(page.content, 'html.parser')                        \n",
    "            \n",
    "#-----------scrape and clean all headlines and append to a list called headlines----------------------------------------------\n",
    "            for header in soup.find_all('h1', class_ ='search-result-story__headline'):\n",
    "                headline = header.get_text().lower()\n",
    "                headline = tokenize(headline)\n",
    "                headlines.append(headline)\n",
    "                for i in headlines:\n",
    "                    for j in i:\n",
    "                        if j in stop_words:\n",
    "                            i.remove(j)    \n",
    "#                 vocabulary = set(headline)\n",
    "#                 tokenized_dict = {word: headline.count(word) for word in vocabulary}\n",
    "#                 filtered_dict = {word: tokenized_dict[word] for word in tokenized_dict if word not in stop_words}\n",
    "\n",
    "#-----------scrape all dates and append to a list called dates---------------------------------------------------------------\n",
    "            for date in soup.find_all('time', class_ = 'published-at'):\n",
    "                date_published = date.get_text()\n",
    "                dates.append(date_published)\n",
    "                \n",
    "                \n",
    "#---convert headlines into tuples--------------------------------------------------------------------------------------------\n",
    "    for idx, headline in enumerate(headlines):\n",
    "        headlines[idx] = tuple(headline)\n",
    "    \n",
    "#---join headlines list and dates list into a list of tuples called data-----------------------------------------------------\n",
    "    data = zip(headlines, dates)\n",
    "    \n",
    "#---remove headlines without these keywords----------------------------------------------------------------------------------\n",
    "    keywords = ['bitcoin', 'cryptocurrency', 'cryptocurrencies', 'crypto', 'blockchain']\n",
    "    filtered_data = filter(lambda x: any(word in x[0] for word in keywords), data)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    cleaned_data = get_headlines()\n",
    "    print(\"%s\" % len(filtered_data) + \" headlines were scraped; stopwords were removed; data written to csv file\")\n",
    "\n",
    "    with open(\"bloomberg.csv\", \"w\") as f:                    \n",
    "        writer=csv.writer(f, delimiter=',')          \n",
    "        writer.writerows(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
