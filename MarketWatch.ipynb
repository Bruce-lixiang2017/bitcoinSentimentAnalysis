{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk, re, string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \n",
    "    # if pos tag starts with 'J'\n",
    "    if pos_tag.startswith('J'):\n",
    "        # return wordnet tag \"ADJ\"\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    # if pos tag starts with 'V'\n",
    "    elif pos_tag.startswith('V'):\n",
    "        # return wordnet tag \"VERB\"\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    # if pos tag starts with 'N'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        # return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # be default, return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "\n",
    "    text= s\n",
    "\n",
    "    pattern= r'[a-zA-Z]+[a-zA-Z\\-\\.]*'                        \n",
    "\n",
    "    tokens=nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "    vocabulary= tokens\n",
    "    return vocabulary;\n",
    "\n",
    "def tokenize_date(s):\n",
    "    pattern=r'[a-z{3}]+[\\.\\s]+[\\d{1,2}\\,\\s]+[\\d{4}]+'                        \n",
    "    tokens=nltk.regexp_tokenize(s, pattern)\n",
    "    date = tokens\n",
    "    return date;\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "headlines=[]\n",
    "    \n",
    "page_url= \"https://www.marketwatch.com/search?q=bitcoin&m=Keyword&rpp=500&mp=806&bd=false&bdv=&rs=false\"\n",
    "\n",
    "while page_url!=None:\n",
    "    page = requests.get(page_url) \n",
    "    \n",
    "    if page.status_code!=200:\n",
    "         page_url=None\n",
    "    else:\n",
    "        all_data = []\n",
    "        \n",
    "        page_url = \"https://www.marketwatch.com/search?q=bitcoin&m=Keyword&rpp=500&mp=806&bd=false&bdv=&rs=false\"\n",
    "        page = requests.get(page_url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", class_ = \"searchresult\")\n",
    "        titles_dictionary = []\n",
    "        for idx, div in enumerate(divs):\n",
    "            titles = div.select(\"a\")\n",
    "            if titles != []:\n",
    "                title = titles[0].get_text().lower()\n",
    "                title = tokenize(title)\n",
    "                tagged_tokens = nltk.pos_tag(title)\n",
    "                le_words =[wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(tag)) \\\n",
    "                           for (word, tag) in tagged_tokens \\\n",
    "                           if word not in stop_words and \\\n",
    "                           word not in string.punctuation]\n",
    "                vocabulary = list(set(le_words))\n",
    "                titles_dictionary.append(vocabulary)\n",
    "        divs_dates = soup.find_all(\"div\", class_ = \"deemphasized\")\n",
    "        dates_list = []\n",
    "        for idx, div in enumerate(divs_dates):\n",
    "            date = div.get_text().lower()\n",
    "            date = str(tokenize_date(date)[0])\n",
    "            dates_list.append(date)\n",
    "        data = zip(titles_dictionary,dates_list)  \n",
    "        all_data.extend(data)\n",
    "    \n",
    "    page_url = None\n",
    "\n",
    "keywords = ['bitcoin','cryptocurrency','cryptocurrencies', 'crypto', 'blockchain']\n",
    "filtered_data = filter(lambda x: any(word in x[0] for word in keywords), all_data)\n",
    "data_frame = pd.DataFrame.from_records(filtered_data, columns = [\"Tokenized Title\", \"Date\"])\n",
    "data_frame.to_csv(\"marketwatch.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
